{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R-UcnvBxZRV"
   },
   "source": [
    "## Sprint 1 -- Regression1, Regression2, Ridge Regression, Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K_NaEANxjPk"
   },
   "source": [
    "### Module 1: Regression1\n",
    "\n",
    "Techniques used:\n",
    "* sys.modules + DATA_PATH variable for Colab (deleted)\n",
    "* %matplotlib inline\n",
    "* pd.options.display.float_format = '{:,.0f}'.format\n",
    "* Plotly express\n",
    "\n",
    "\n",
    "Topics covered:\n",
    "* mean baseline\n",
    "* feature matrix, target vector\n",
    "* fit and predict with models\n",
    "* error metrics\n",
    "* model.coef_ and model.intercept_ for LinearRegression object\n",
    "* write prediction function to use model on new data\n",
    "\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "Regression outputs continuous numbers\n",
    "\n",
    "Classification outputs categories\n",
    "\n",
    "Baseline might mean a fast first model, human-level performance, worst model that's good enough for production...\n",
    "\n",
    "Classical programming knows the rules and figures out the answers.   \n",
    "Machine learning knows the answers and figures out the rules.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHWl6wJ02rx5"
   },
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tViuGdjZ2kLu"
   },
   "source": [
    "### Module 2: Regression2 (multiple regression)\n",
    "\n",
    "Techniques used:\n",
    "* 3d Plotly express, writes function to plot plane of best fit\n",
    "* handling multiple model.coef_'s  --> .to_string()\n",
    "* model.predict([[2, 100]]) takes in the coefs\n",
    "\n",
    "\n",
    "Topics covered:\n",
    "* train, test split -- random or time series, manually or by library\n",
    "* X feature matrix, y target vector\n",
    "* baseline model, train and test error\n",
    "* LinearRegression() but w more features\n",
    "* squared errors demo\n",
    "* overfitting, underfitting (bias/variance tradeoff)\n",
    "* Fancy PolynomialRegression code from jakevdp to fit multiple models at different degrees of polynomial\n",
    "\n",
    "Notes:\n",
    "\n",
    "Too much bias underfits the data.   \n",
    "\n",
    "But attempting to explain all the variance overfits, because the model learns the signal AND the noise (random error)\n",
    "\n",
    "Using different degrees of polynomial is a form of model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpFyX28s_nQc"
   },
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0wKPo4N_nGL"
   },
   "source": [
    "### Module 3: Ridge Regression \n",
    "\n",
    "Techniques used:\n",
    "* filter data by percentiles\n",
    "* \"n choose k\" code\n",
    "* pd.to_datetime(), time series cutoffs\n",
    "* SelectKbest, f_regression\n",
    "* selector.get_support() to access Selector stuff like which features got selected\n",
    "* from IPython.display import display, HTML to print pretty text above matplotlib subplots\n",
    "\n",
    "\n",
    "Topics covered:\n",
    "* One-hot encoding, train and test\n",
    "* Feature selection (SelectKBest)\n",
    "* .fit_transform() on train, .transform() on test\n",
    "* Some light Feature Engineering on the apartments dataset\n",
    "* For loop, model all k's in range(1,n_features) and print their scores\n",
    "* Ridge()\n",
    "* RidgeCV - get the best alpha\n",
    "* Demo: for alpha in list, plot coef chart\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "Our Scikit learn models expect all numbers, so we encode (low-cardinality) categorical strings\n",
    "\n",
    "Feature selection is a big deal\n",
    "\n",
    "There are n_features! combinations of features\n",
    "\n",
    "Univariate feature selection uses univariate statistics to pick features.\n",
    "* You can SelectKBest, SelectPercentile of best features, etc.\n",
    "* There are other types (like permutation importance). Details here https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "Regularization to add just enough bias that models don't go crazy.\n",
    "  * alphas range from =OLS line to =mean baseline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZnfcU7LRIWI"
   },
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzIVw96HRIS3"
   },
   "source": [
    "### Module 4: Logistic Regression \n",
    "\n",
    "Techniques used:\n",
    "* log_reg.coef_, .intercept_, .predict, .predict_proba\n",
    "* 2 ways to score:\n",
    "  * y_pred = model.predidct(X_val_scaled) --> accuracy_score(y_val, y_pred)\n",
    "  * model.score(X_val_scaled, y_val)\n",
    "\n",
    "\n",
    "Topics covered:\n",
    "* train/val/test split\n",
    "* majority class baseline\n",
    "* LogisticRegression() for baseline classification\n",
    "* Evaluation metric - accuracy_score\n",
    "* math breakdown - the sigmoid function\n",
    "* first good full pipeline -- encode, impute, scale, fit\n",
    "* LogisticRegressionCV()\n",
    "* plot coefficients\n",
    "* Kaggle submission code .to_csv()\n",
    "\n",
    "\n",
    "\n",
    "Notes:  \n",
    "There are 9 and 12 minute Aaron videos for the math of logistic regression\n",
    "\n",
    "Squishification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpYniNrLn_nk"
   },
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXh5Iq00oBe5"
   },
   "source": [
    "## Sprint 2 (Kaggle) -- Decision Trees, Random Forests, CrossVal, Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGdLZp6RoMyw"
   },
   "source": [
    "### Module 1: Decision Trees\n",
    "\n",
    "Techniques used:\n",
    "* pandas_profiling to find issues with the data\n",
    "* Plotly express - mapbox scatter\n",
    "* graphviz\n",
    "* itertools, cool function visualizing LogReg predictions vs DecisionTree ones\n",
    "\n",
    "\n",
    "Topics covered:\n",
    "* Wrangle train-test-val the same way w a function\n",
    "  * Pass in X, make X.copy(), replace 0's w NaNs, drop redundant columns, return X\n",
    "* various feature selections -- i.e. numericals + categoricals under 50 cardinality\n",
    "* Pipelines to combine steps -- Encode, Impute, Scale, Fit, Predict\n",
    "* Get and plot coefficients -- harder w pipelines (no .coef_)\n",
    "  * access \"named_steps\"\n",
    "* sklearn DecisionTreeClassifier()\n",
    "* .feature_importances_\n",
    "* demos visualizing water pump and other predictions w different max depths\n",
    "\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "Reference resource: [The Quartz Guide to Bad Data](https://github.com/Quartz/bad-data-guide)\n",
    "\n",
    "Split into train, val if test is already provided\n",
    "\n",
    "In a pipeline, can grid search all the parameters at once\n",
    "\n",
    "Pipelines add safety because you have to go into the pipeline to mess anything up.\n",
    "\n",
    "Linear models have coefficients; trees have feature importances\n",
    "\n",
    "Assignment notebook has good \"real-worldish\" recommendations to explore data. It's feisty coding.\n",
    "  * replace less common cardinalities with \"OTHER\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RHucqrEoR8c"
   },
   "source": [
    "### Module 2: Random Forests\n",
    "\n",
    "Techniques used:\n",
    "* asdf\n",
    "\n",
    "\n",
    "Topics covered:\n",
    "* split, wrangle as before\n",
    "* RandomForestClassifier() - pipeline\n",
    "* accessing things after pipeline\n",
    "  * encoder = named_steps['onehotencoder'] --> encoded = encoder.transform(X_train)\n",
    "* model.feature_importances_\n",
    "* ce.OrdinalEncoder()\n",
    "* extremely randomized trees - ExtraTreesClassifier, ExtraTreesRegressor\n",
    "* big demo - LogReg, DecTree, RandForest.\n",
    "  * interactive widget sliders and model comparisons\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "When you want to access something that's in a pipeline, you have to use pipeline.named_steps['nameinalllowercase']\n",
    "\n",
    "Random forest is bagging - \"bootstrap aggregating\" (which just means lots of small samples w replacement).  \n",
    "Gradient boost is boosting\n",
    "  * the former may be less sensitive to hyperparameters, the latter might score better once fit well\n",
    "\n",
    "ordinal encoding lets you use high-card cats\n",
    "\n",
    "With ordinal encoding, \"you can optionally pass in a mapping dict to give order to the classes rather than selecting integers at random -- this assumes the classes have inherent order\"\n",
    "\n",
    "selectKbest is for linear models\n",
    "\n",
    "permutation importances for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URqU2mpCoRhe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Unit_2_summary.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
