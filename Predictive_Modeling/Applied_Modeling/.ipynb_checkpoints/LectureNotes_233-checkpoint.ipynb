{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2ha9OWxf0jw"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 3, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hTictxWYih7"
   },
   "source": [
    "# Permutation Importances & Gradient Boosting\n",
    "\n",
    "- Get **permutation importances** for model interpretation and feature selection\n",
    "- Use xgboost for **gradient boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture tl;dr\n",
    "\n",
    "# Feature importances are fast, but permutation importances can be better.\n",
    "#\n",
    "# Feature importances are like a greedy algorithm - one feature dominates\n",
    "# because any information it provides is taken out of the pool for the rest.\n",
    "# Therefore it will look highly predictive while others won't.\n",
    "# Not a big deal if you're doing feature selection to reduce overfitting (you\n",
    "# want to reduce redundancy anyway), but it's not strictly accurate.\n",
    "#\n",
    "# Permutation importance fixes this.\n",
    "\n",
    "# Reading on this in the scikitlearn Random Forest implementation. \"Gini decreases\"\n",
    "\n",
    "# Theoretically, drop-column importances are the best way to determine feature importance.\n",
    "# You run n - 1 pipelines and compare scores. But it's too slow.\n",
    "\n",
    "# So in order to not have to retrain the estimator for every feature, permutation importance\n",
    "# works by replacing each feature's values with random noise and showing THAT drop in\n",
    "# the eval metric. (noise values must be in same value distribution as original feature)\n",
    "\n",
    "# Code for perm importances w eli5 library\n",
    "\n",
    "# can use for feature selection by removing columns w low importance\n",
    "\n",
    "# Gradient Boosting works more like a neural net than Random Forest bootstrapping in that\n",
    "# trees are built sequentially off of the previous tree, and thus learns slowly.\n",
    "\n",
    "# Notes on XGBoost libraries intricacies and suggestions for hyperparameters to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "\n",
    "- Permutation Importances\n",
    "  - [Kaggle / Dan Becker: Machine Learning Explainability](https://www.kaggle.com/dansbecker/permutation-importance)\n",
    "  - [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
    "- (Default) Feature Importances\n",
    "  - [Ando Saabas: Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
    "  - [Terence Parr, et al: Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
    "  \n",
    "- [Selecting good features – Part III: random forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/) \n",
    "\n",
    "- [Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
    "\n",
    "- [ELI5 library docs - Permutation Importance,](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)\n",
    "\n",
    "For more documentation on using this library, see:\n",
    "- [eli5.sklearn.PermutationImportance](https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#eli5.sklearn.permutation_importance.PermutationImportance)\n",
    "- [eli5.show_weights](https://eli5.readthedocs.io/en/latest/autodocs/eli5.html#eli5.show_weights)\n",
    "- [scikit-learn user guide, `scoring` parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)\n",
    "\n",
    "eli5 doesn't work with pipelines.\n",
    "\n",
    "[_An Introduction to Statistical Learning_Chapter 8.2.3, Boosting:](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMSDBzRzSsNQ"
   },
   "source": [
    "#### Python libraries for Gradient Boosting\n",
    "- [scikit-learn Gradient Tree Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) — slower than other libraries, but [the new version may be better](https://twitter.com/amuellerml/status/1129443826945396737)\n",
    "  - Anaconda: already installed\n",
    "  - Google Colab: already installed\n",
    "- [xgboost](https://xgboost.readthedocs.io/en/latest/) — can accept missing values and enforce [monotonic constraints](https://xiaoxiaowang87.github.io/monotonicity_constraint/)\n",
    "  - Anaconda, Mac/Linux: `conda install -c conda-forge xgboost`\n",
    "  - Windows: `conda install -c anaconda py-xgboost`\n",
    "  - Google Colab: already installed\n",
    "- [LightGBM](https://lightgbm.readthedocs.io/en/latest/) — can accept missing values and enforce [monotonic constraints](https://blog.datadive.net/monotonicity-constraints-in-machine-learning/)\n",
    "  - Anaconda: `conda install -c conda-forge lightgbm`\n",
    "  - Google Colab: already installed\n",
    "- [CatBoost](https://catboost.ai/) — can accept missing values and use [categorical features](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html) without preprocessing\n",
    "  - Anaconda: `conda install -c conda-forge catboost`\n",
    "  - Google Colab: `pip install catboost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMejJg0w8v76"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the code cell below. You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab.\n",
    "\n",
    "Libraries:\n",
    "\n",
    "- category_encoders\n",
    "- [**eli5**](https://eli5.readthedocs.io/en/latest/)\n",
    "- matplotlib\n",
    "- numpy\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- [**xgboost**](https://xgboost.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BFQMky3CYih-"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'\n",
    "# %%capture\n",
    "# import sys\n",
    "\n",
    "# # If you're on Colab:\n",
    "# if 'google.colab' in sys.modules:\n",
    "#     DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'\n",
    "#     !pip install category_encoders==2.*\n",
    "#     !pip install eli5\n",
    "\n",
    "# # If you're working locally:\n",
    "# else:\n",
    "#     DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdHmjhC0SsMs"
   },
   "source": [
    "We'll go back to Tanzania Waterpumps for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "z-TExplb_Slf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Merge train_features.csv & train_labels.csv\n",
    "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n",
    "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
    "\n",
    "# Read test_features.csv & sample_submission.csv\n",
    "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
    "\n",
    "\n",
    "# Split train into train & val\n",
    "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
    "                              stratify=train['status_group'], random_state=42)\n",
    "\n",
    "\n",
    "def wrangle(X):\n",
    "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
    "    \n",
    "    # Prevent SettingWithCopyWarning\n",
    "    X = X.copy()\n",
    "    \n",
    "    # About 3% of the time, latitude has small values near zero,\n",
    "    # outside Tanzania, so we'll treat these values like zero.\n",
    "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
    "    \n",
    "    # When columns have zeros and shouldn't, they are like null values.\n",
    "    # So we will replace the zeros with nulls, and impute missing values later.\n",
    "    # Also create a \"missing indicator\" column, because the fact that\n",
    "    # values are missing may be a predictive signal.\n",
    "    cols_with_zeros = ['longitude', 'latitude', 'construction_year', \n",
    "                       'gps_height', 'population']\n",
    "    for col in cols_with_zeros:\n",
    "        X[col] = X[col].replace(0, np.nan)\n",
    "        X[col+'_MISSING'] = X[col].isnull()\n",
    "            \n",
    "    # Drop duplicate columns\n",
    "    duplicates = ['quantity_group', 'payment_type']\n",
    "    X = X.drop(columns=duplicates)\n",
    "    \n",
    "    # Drop recorded_by (never varies) and id (always varies, random)\n",
    "    unusable_variance = ['recorded_by', 'id']\n",
    "    X = X.drop(columns=unusable_variance)\n",
    "    \n",
    "    # Convert date_recorded to datetime\n",
    "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
    "    \n",
    "    # Extract components from date_recorded, then drop the original column\n",
    "    X['year_recorded'] = X['date_recorded'].dt.year\n",
    "    X['month_recorded'] = X['date_recorded'].dt.month\n",
    "    X['day_recorded'] = X['date_recorded'].dt.day\n",
    "    X = X.drop(columns='date_recorded')\n",
    "    \n",
    "    # Engineer feature: how many years from construction_year to date_recorded\n",
    "    X['years'] = X['year_recorded'] - X['construction_year']\n",
    "    X['years_MISSING'] = X['years'].isnull()\n",
    "    \n",
    "    # return the wrangled dataframe\n",
    "    return X\n",
    "\n",
    "train = wrangle(train)\n",
    "val = wrangle(val)\n",
    "test = wrangle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rhg8PQKt_jzP"
   },
   "outputs": [],
   "source": [
    "# Arrange data into X features matrix and y target vector\n",
    "target = 'status_group'\n",
    "X_train = train.drop(columns=target)\n",
    "y_train = train[target]\n",
    "X_val = val.drop(columns=target)\n",
    "y_val = val[target]\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "m8lB4z5l_eml",
    "outputId": "156c2764-c2b1-4002-c6c6-dbee3a451680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.8135521885521886\n"
     ]
    }
   ],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median'), \n",
    "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# Fit on train, score on val\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HS7cHuPSsM2"
   },
   "source": [
    "# Get permutation importances for model interpretation and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87qPcvzeSsM3"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuUBKCP4SsM4"
   },
   "source": [
    "Default Feature Importances are fast, but Permutation Importances may be more accurate.\n",
    "\n",
    "These links go deeper with explanations and examples:\n",
    "\n",
    "- Permutation Importances\n",
    "  - [Kaggle / Dan Becker: Machine Learning Explainability](https://www.kaggle.com/dansbecker/permutation-importance)\n",
    "  - [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
    "- (Default) Feature Importances\n",
    "  - [Ando Saabas: Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
    "  - [Terence Parr, et al: Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HOayKBOYiit"
   },
   "source": [
    "There are three types of feature importances:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bRhsxENYiiu"
   },
   "source": [
    "### 1. (Default) Feature Importances\n",
    "\n",
    "Fastest, good for first estimates, but be aware:\n",
    "\n",
    "\n",
    "\n",
    ">**When the dataset has two (or more) correlated features, then from the point of view of the model, any of these correlated features can be used as the predictor, with no concrete preference of one over the others.** But once one of them is used, the importance of others is significantly reduced since effectively the impurity they can remove is already removed by the first feature. As a consequence, they will have a lower reported importance. This is not an issue when we want to use feature selection to reduce overfitting, since it makes sense to remove features that are mostly duplicated by other features. But when interpreting the data, it can lead to the incorrect conclusion that one of the variables is a strong predictor while the others in the same group are unimportant, while actually they are very close in terms of their relationship with the response variable. — [Selecting good features – Part III: random forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/) \n",
    "\n",
    "\n",
    " \n",
    " > **The scikit-learn Random Forest feature importance ... tends to inflate the importance of continuous or high-cardinality categorical variables.** ... Breiman and Cutler, the inventors of Random Forests, indicate that this method of “adding up the gini decreases for each individual variable over all trees in the forest gives a **fast** variable importance that is often very consistent with the permutation importance measure.” —  [Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "BNVm6f7mYiiu",
    "outputId": "fce34248-59e0-40ce-94d9-8591a7cd073e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get feature importances\n",
    "rf = pipeline.named_steps['randomforestclassifier']\n",
    "importances = pd.Series(rf.feature_importances_, X_train.columns)\n",
    "\n",
    "# Plot feature importances\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 20\n",
    "plt.figure(figsize=(10,n/2))\n",
    "plt.title(f'Top {n} features')\n",
    "importances.sort_values()[-n:].plot.barh(color='grey');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8HzLcCBYiiv"
   },
   "source": [
    "### 2. Drop-Column Importance\n",
    "\n",
    "The best in theory, but too slow in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "DQAOlERnYiiw",
    "outputId": "7b5f131f-8616-4d51-cd43-3304ca6151d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy without quantity: 0.7771043771043771\n",
      "Validation Accuracy with quantity: 0.8135521885521886\n",
      "Drop-Column Importance for quantity: 0.03644781144781151\n"
     ]
    }
   ],
   "source": [
    "column  = 'quantity'\n",
    "\n",
    "# Fit without column\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median'), \n",
    "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "pipeline.fit(X_train.drop(columns=column), y_train)\n",
    "score_without = pipeline.score(X_val.drop(columns=column), y_val)\n",
    "print(f'Validation Accuracy without {column}: {score_without}')\n",
    "\n",
    "# Fit with column\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median'), \n",
    "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "score_with = pipeline.score(X_val, y_val)\n",
    "print(f'Validation Accuracy with {column}: {score_with}')\n",
    "\n",
    "# Compare the error with & without column\n",
    "print(f'Drop-Column Importance for {column}: {score_with - score_without}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Vu39wGkYiix"
   },
   "source": [
    "### 3. Permutation Importance\n",
    "\n",
    "Permutation Importance is a good compromise between Feature Importance based on impurity reduction (which is the fastest) and Drop Column Importance (which is the \"best.\")\n",
    "\n",
    "[The ELI5 library documentation explains,](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)\n",
    "\n",
    "> Importance can be measured by looking at how much the score (accuracy, F1, R^2, etc. - any score we’re interested in) decreases when a feature is not available.\n",
    ">\n",
    "> To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. ...\n",
    ">\n",
    ">To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn’t work as-is, because estimators expect feature to be present. So instead of removing a feature we can replace it with random noise - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the same distribution as original feature values (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples’ feature values - this is how permutation importance is computed.\n",
    ">\n",
    ">The method is most suitable for computing feature importances when a number of columns (features) is not huge; it can be resource-intensive otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYCiEx7zYiiy"
   },
   "source": [
    "### Do-It-Yourself way, for intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "TksOf_n2Yiiy",
    "outputId": "a51a0292-929e-46ae-c0f1-fb3545d25612"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3290     insufficient\n",
       "47666    insufficient\n",
       "2538           enough\n",
       "53117          enough\n",
       "51817          enough\n",
       "Name: quantity, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEFORE: Sequence of the feature to be permuted\n",
    "feature = 'quantity'\n",
    "X_val[feature].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "QEdATeU-h5Da",
    "outputId": "9c14210e-d116-4939-cd0c-da186bcf99f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enough          6619\n",
       "insufficient    2976\n",
       "dry             1325\n",
       "seasonal         806\n",
       "unknown          154\n",
       "Name: quantity, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEFORE: Distribution of the feature to be permuted\n",
    "X_val[feature].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4lCMmmURiAas"
   },
   "outputs": [],
   "source": [
    "# PERMUTE!\n",
    "X_val_permuted = X_val.copy()\n",
    "X_val_permuted[feature] = np.random.permutation(X_val[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "jj8moRjxiUqq",
    "outputId": "42482a72-4507-4fef-e105-16aa8abcf66c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3290     insufficient\n",
       "47666          enough\n",
       "2538           enough\n",
       "53117          enough\n",
       "51817          enough\n",
       "Name: quantity, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AFTER: Sequence has changed\n",
    "X_val_permuted[feature].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "1ne1fFnyijpA",
    "outputId": "c383a7d8-a40d-47e9-d938-3978b72241f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enough          6619\n",
       "insufficient    2976\n",
       "dry             1325\n",
       "seasonal         806\n",
       "unknown          154\n",
       "Name: quantity, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AFTER: Distribution hasn't changed\n",
    "X_val_permuted[feature].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "QpA1jmWTiryd",
    "outputId": "63cd56c5-a6bd-454a-df36-b83ed7e67c86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with quantity: 0.8135521885521886\n",
      "Validation Accuracy with quantity permuted: 0.7140572390572391\n",
      "Permutation Importance: 0.09949494949494953\n"
     ]
    }
   ],
   "source": [
    "# Get the permutation importance\n",
    "# Notice that we don't need to refit here!\n",
    "score_permuted = pipeline.score(X_val_permuted, y_val)\n",
    "\n",
    "print(f'Validation Accuracy with {feature}: {score_with}')\n",
    "print(f'Validation Accuracy with {feature} permuted: {score_permuted}')\n",
    "print(f'Permutation Importance: {score_with - score_permuted}')\n",
    "\n",
    "# Permutation importance = (score with feature) - (score w/o feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "MzNZSD6hjD7o",
    "outputId": "863ab4a9-9b8f-473f-cc88-9f4b77db7213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with wpt_name: 0.8135521885521886\n",
      "Validation Accuracy with wpt_name permuted: 0.8125420875420876\n",
      "Permutation Importance: 0.0010101010101010166\n"
     ]
    }
   ],
   "source": [
    "# Rerun the permutation importance process, \n",
    "# but for a different feature\n",
    "feature = 'wpt_name'\n",
    "X_val_permuted = X_val.copy()\n",
    "X_val_permuted[feature] = np.random.permutation(X_val[feature])\n",
    "score_permuted = pipeline.score(X_val_permuted, y_val)\n",
    "\n",
    "print(f'Validation Accuracy with {feature}: {score_with}')\n",
    "print(f'Validation Accuracy with {feature} permuted: {score_permuted}')\n",
    "print(f'Permutation Importance: {score_with - score_permuted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LYk19SNYii7"
   },
   "source": [
    "### With eli5 library\n",
    "\n",
    "For more documentation on using this library, see:\n",
    "- [eli5.sklearn.PermutationImportance](https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#eli5.sklearn.permutation_importance.PermutationImportance)\n",
    "- [eli5.show_weights](https://eli5.readthedocs.io/en/latest/autodocs/eli5.html#eli5.show_weights)\n",
    "- [scikit-learn user guide, `scoring` parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)\n",
    "\n",
    "eli5 doesn't work with pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "ZL0xwJ5pn6sx",
    "outputId": "4338f7c0-68e8-4b46-d8b1-10edb0cbdcb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median')\n",
    ")\n",
    "\n",
    "X_train_transformed = transformers.fit_transform(X_train)\n",
    "X_val_transformed = transformers.transform(X_val)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "hpSemTkFFP8i",
    "outputId": "7afa821d-94b9-4761-da71-4655d99654c9"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'eli5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5fa6738b7303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ignore warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPermutationImportance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'eli5'"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "permuter = PermutationImportance(\n",
    "    model, \n",
    "    scoring='accuracy', \n",
    "    n_iter=5, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "permuter.fit(X_val_transformed, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "h6xm7OUZpXH_",
    "outputId": "6d03ea9a-8efa-4f5c-d82c-3dbf99588d0b"
   },
   "outputs": [],
   "source": [
    "# How many calculations?\n",
    "# Random Forest, n_estimators=100, max_depth=20\n",
    "# Validation dataset: 10,000 observations\n",
    "# 50 features\n",
    "# Permutation Importance n_iter=5\n",
    "100 * 20 * 10000 * 50 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815
    },
    "id": "XweYGVpxpdba",
    "outputId": "43939ec4-8129-485a-ca3a-ee2ce50688f0"
   },
   "outputs": [],
   "source": [
    "feature_names = X_val.columns.tolist()\n",
    "pd.Series(permuter.feature_importances_, feature_names).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815
    },
    "id": "9Ga0B2-5o9pC",
    "outputId": "8d9b87fc-aba1-41b6-f75c-64bc6bd62605"
   },
   "outputs": [],
   "source": [
    "# 2. Display permutation importances\n",
    "eli5.show_weights(\n",
    "    permuter, \n",
    "    top=None, # No limit: show permutation importances for all features\n",
    "    feature_names=feature_names # must be a list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q07yW9k-Yii8"
   },
   "source": [
    "### We can use importances for feature selection\n",
    "\n",
    "For example, we can remove features with zero importance. The model trains faster and the score does not decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tZrPFyEMYii9",
    "outputId": "6fbf89f1-96c4-4939-b3f8-d84d9ae9fbb6"
   },
   "outputs": [],
   "source": [
    "print('Shape before removing features:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f6t30f-rb61"
   },
   "outputs": [],
   "source": [
    "minimum_importance = 0\n",
    "mask = permuter.feature_importances_ > minimum_importance\n",
    "features = X_train.columns[mask]\n",
    "X_train = X_train[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "AQAEuwBCsR5a",
    "outputId": "26e1fdc1-8015-44a4-add3-20cba3b7dc7b"
   },
   "outputs": [],
   "source": [
    "print('Shape after removing faetures:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "qC8nHgq1sVxy",
    "outputId": "5fd41d0c-11d7-4472-9019-3bb46c21f010"
   },
   "outputs": [],
   "source": [
    "X_val = X_val[features]\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median'), \n",
    "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# Fit on train, score on val\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "8OqjvsCUszrI",
    "outputId": "2c32d9df-7489-43e3-8b66-3f9bd5c83d23"
   },
   "outputs": [],
   "source": [
    "# You could try something like this to remove more features if you want:\n",
    "permuter.feature_importances_ - permuter.feature_importances_std_ > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fl67bCR7WY6j"
   },
   "source": [
    "# Use xgboost for gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY6x7_SISsNO"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3USHJYK3SsNO"
   },
   "source": [
    "In the Random Forest lesson, you learned this advice:\n",
    "\n",
    "#### Try Tree Ensembles when you do machine learning with labeled, tabular data\n",
    "- \"Tree Ensembles\" means Random Forest or **Gradient Boosting** models. \n",
    "- [Tree Ensembles often have the best predictive accuracy](https://arxiv.org/abs/1708.05070) with labeled, tabular data.\n",
    "- Why? Because trees can fit non-linear, non-[monotonic](https://en.wikipedia.org/wiki/Monotonic_function) relationships, and [interactions](https://christophm.github.io/interpretable-ml-book/interaction.html) between features.\n",
    "- A single decision tree, grown to unlimited depth, will [overfit](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/). We solve this problem by ensembling trees, with bagging (Random Forest) or **[boosting](https://www.youtube.com/watch?v=GM3CDQfQ4sw)** (Gradient Boosting).\n",
    "- Random Forest's advantage: may be less sensitive to hyperparameters. **Gradient Boosting's advantage:** may get better predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsWC_JAmSsNP"
   },
   "source": [
    "Like Random Forest, Gradient Boosting uses ensembles of trees. But the details of the ensembling technique are different:\n",
    "\n",
    "### Understand the difference between boosting & bagging\n",
    "\n",
    "Boosting (used by Gradient Boosting) is different than Bagging (used by Random Forests). \n",
    "\n",
    "Here's an excerpt from [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) Chapter 8.2.3, Boosting:\n",
    "\n",
    ">Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.\n",
    ">\n",
    ">**Boosting works in a similar way, except that the trees are grown _sequentially_: each tree is grown using information from previously grown trees.**\n",
    ">\n",
    ">Unlike fitting a single large decision tree to the data, which amounts to _fitting the data hard_ and potentially overfitting, the boosting approach instead _learns slowly._ Given the current model, we fit a decision tree to the residuals from the model.\n",
    ">\n",
    ">We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes. **By fitting small trees to the residuals, we slowly improve fˆ in areas where it does not perform well.**\n",
    ">\n",
    ">Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.\n",
    "\n",
    "This high-level overview is all you need to know for now. If you want to go deeper, we recommend you watch the StatQuest videos on gradient boosting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMSDBzRzSsNQ"
   },
   "source": [
    "Let's write some code. We have lots of options for which libraries to use:\n",
    "\n",
    "#### Python libraries for Gradient Boosting\n",
    "- [scikit-learn Gradient Tree Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) — slower than other libraries, but [the new version may be better](https://twitter.com/amuellerml/status/1129443826945396737)\n",
    "  - Anaconda: already installed\n",
    "  - Google Colab: already installed\n",
    "- [xgboost](https://xgboost.readthedocs.io/en/latest/) — can accept missing values and enforce [monotonic constraints](https://xiaoxiaowang87.github.io/monotonicity_constraint/)\n",
    "  - Anaconda, Mac/Linux: `conda install -c conda-forge xgboost`\n",
    "  - Windows: `conda install -c anaconda py-xgboost`\n",
    "  - Google Colab: already installed\n",
    "- [LightGBM](https://lightgbm.readthedocs.io/en/latest/) — can accept missing values and enforce [monotonic constraints](https://blog.datadive.net/monotonicity-constraints-in-machine-learning/)\n",
    "  - Anaconda: `conda install -c conda-forge lightgbm`\n",
    "  - Google Colab: already installed\n",
    "- [CatBoost](https://catboost.ai/) — can accept missing values and use [categorical features](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html) without preprocessing\n",
    "  - Anaconda: `conda install -c conda-forge catboost`\n",
    "  - Google Colab: `pip install catboost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qB87nVpSsNR"
   },
   "source": [
    "In this lesson, you'll use a new library, xgboost — But it has an API that's almost the same as scikit-learn, so it won't be a hard adjustment!\n",
    "\n",
    "#### [XGBoost Python API Reference: Scikit-Learn API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "wsnJRKjfWYph",
    "outputId": "38f3aae4-c191-443a-cb30-bbcf074c5442"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5yYIdJDRzHjH",
    "outputId": "017236cb-47c1-4543-975b-f458be72ade9"
   },
   "outputs": [],
   "source": [
    "# The score will be worse, but we haven't tuned hyperparameters yet\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = pipeline.predict(X_val)\n",
    "print('Validation Accuracy', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCjVSlD_XJr2"
   },
   "source": [
    "#### [Avoid Overfitting By Early Stopping With XGBoost In Python](https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/)\n",
    "\n",
    "Why is early stopping better than a For loop, or GridSearchCV, to optimize `n_estimators`?\n",
    "\n",
    "With early stopping, if `n_iterations` is our number of iterations, then we fit `n_iterations` decision trees.\n",
    "\n",
    "With a for loop, or GridSearchCV, we'd fit `sum(range(1,n_rounds+1))` trees.\n",
    "\n",
    "But it doesn't work well with pipelines. You may need to re-run multiple times with different values of other parameters such as `max_depth` and `learning_rate`.\n",
    "\n",
    "#### XGBoost parameters\n",
    "- [Notes on parameter tuning](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html)\n",
    "- [Parameters documentation](https://xgboost.readthedocs.io/en/latest/parameter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ZNX3IKftXBFS",
    "outputId": "a085f37e-b19e-43d4-a428-8505763a0f7c"
   },
   "outputs": [],
   "source": [
    "# 1 + 2 + 3 + ... + 1000\n",
    "sum(range(1,1000+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6i-avVMD0R-H",
    "outputId": "34dad874-4d6b-4b8e-8d87-470957bfb0a9"
   },
   "outputs": [],
   "source": [
    "encoder = ce.OrdinalEncoder()\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_val_encoded = encoder.transform(X_val)\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=1000, # <= 1000 trees, depend on early stopping\n",
    "    max_depth=7,       # try deeper trees because of high cardinality categoricals\n",
    "    learning_rate=0.5, # try higher learning rate\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "eval_set = [(X_train_encoded, y_train), \n",
    "            (X_val_encoded, y_val)]\n",
    "\n",
    "model.fit(X_train_encoded, y_train, \n",
    "          eval_set=eval_set, \n",
    "          eval_metric='merror', \n",
    "          early_stopping_rounds=50) # Stop if the score hasn't improved in 50 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "qQdOuj_C28Gd",
    "outputId": "bf722f0b-8466-45dd-de4f-51e06c12bd97"
   },
   "outputs": [],
   "source": [
    "results = model.evals_result()\n",
    "train_error = results['validation_0']['merror']\n",
    "val_error = results['validation_1']['merror']\n",
    "epoch = list(range(1, len(train_error)+1))\n",
    "plt.plot(epoch, train_error, label='Train')\n",
    "plt.plot(epoch, val_error, label='Validation')\n",
    "plt.ylabel('Classification Error')\n",
    "plt.xlabel('Model Complexity (n_estimators)')\n",
    "plt.title('Validation Curve for this XGBoost model')\n",
    "plt.ylim((0.18, 0.22)) # Zoom in\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF7-ml6BhRRf"
   },
   "source": [
    "### Try adjusting these hyperparameters\n",
    "\n",
    "#### Random Forest\n",
    "- class_weight (for imbalanced classes)\n",
    "- max_depth (usually high, can try decreasing)\n",
    "- n_estimators (too low underfits, too high wastes time)\n",
    "- min_samples_leaf (increase if overfitting)\n",
    "- max_features (decrease for more diverse trees)\n",
    "\n",
    "#### Xgboost\n",
    "- scale_pos_weight (for imbalanced classes)\n",
    "- max_depth (usually low, can try increasing)\n",
    "- n_estimators (too low underfits, too high wastes time/overfits) — Use Early Stopping!\n",
    "- learning_rate (too low underfits, too high overfits)\n",
    "\n",
    "For more ideas, see [Notes on Parameter Tuning](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html) and [DART booster](https://xgboost.readthedocs.io/en/latest/tutorials/dart.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHZs5A49SsNZ"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will use your portfolio project dataset for all assignments this sprint. Complete these tasks for your project, and document your work.\n",
    "\n",
    "- Continue to clean and explore your data. Make exploratory visualizations.\n",
    "- Fit a model. Does it beat your baseline?\n",
    "- Try xgboost.\n",
    "- Get your model's permutation importances.\n",
    "\n",
    "You should try to complete an initial model today, because the rest of the week, we're making model interpretation visualizations.\n",
    "\n",
    "But, if you aren't ready to try xgboost and permutation importances with your dataset today, you can practice with another dataset instead. You may choose any dataset you've worked with previously."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LectureNotes_233.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
