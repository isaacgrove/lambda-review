{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R-UcnvBxZRV"
   },
   "source": [
    "## Sprint 1 -- Regression1, Regression2, Ridge Regression, Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K_NaEANxjPk"
   },
   "source": [
    "### Module 1: Regression1\n",
    "\n",
    "Techniques used:\n",
    "* sys.modules + DATA_PATH to import data\n",
    "* %matplotlib inline\n",
    "* pd.options.display.float_format = '{:,.0f}'.format\n",
    "* px.scatter (trendline='ols')\n",
    "\n",
    "\n",
    "Topics covered:\n",
    "* mean baseline\n",
    "* feature matrix, target vector\n",
    "* fit and predict with models\n",
    "* error metrics\n",
    "* model.coef_ and model.intercept_ for LinearRegression object\n",
    "* write prediction function to use model on new data\n",
    "\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "Regression models output continuous numbers\n",
    "\n",
    "\n",
    "Baseline can mean different things -- fast first model that beats guessing, human-level performance, worst model that's good enough for production...\n",
    "\n",
    "Classical programming gets the answers.   \n",
    "Machine learning gets the rules.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHWl6wJ02rx5"
   },
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tViuGdjZ2kLu"
   },
   "source": [
    "### Module 2: Regression2 (multiple regression)\n",
    "\n",
    "Techniques used:\n",
    "* 3d Plotly express, function to plot plane of best fit\n",
    "* handling multiple model.coef_'s  --> .to_string()\n",
    "* model.predict([[2, 100]]) takes in the coefs\n",
    "\n",
    "\n",
    "Topics covered:\n",
    "* train, test split -- random or time series, manually or by library\n",
    "* X feature matrix, y target vector\n",
    "* baseline model, train and test error thereof\n",
    "* LinearRegression() model w more features\n",
    "* squared errors demo\n",
    "* overfitting, underfitting (the bias/variance tradeoff)\n",
    "* Fancy PolynomialRegression code from jakevdp to fit multiple models at different degrees of polynomial\n",
    "\n",
    "Notes:\n",
    "\n",
    "Lots of bias underfits the data.   \n",
    "High variance = \"attempting to explain all the variance\" overfits because the model ends up accounting for random errors.\n",
    "\n",
    "Using different degrees of polynomial is a form of model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpFyX28s_nQc"
   },
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0wKPo4N_nGL"
   },
   "source": [
    "### Module 3: Ridge Regression \n",
    "\n",
    "Techniques used:\n",
    "* filter data by percentiles\n",
    "* pd.to_datetime(), time series cutoffs\n",
    "* SelectKbest, f_regression\n",
    "* selector.get_support() to access Selector stuff like which features got selected\n",
    "* from IPython.display import display, HTML to print pretty text above matplotlib subplots\n",
    "\n",
    "\n",
    "Topics covered:\n",
    "* One-hot encoding, train and test\n",
    "* Feature selection (SelectKBest)\n",
    "* .fit_transform() on train, .transform() on test\n",
    "* Some light Feature Engineering on the apartments dataset\n",
    "* For loop, model all k's in range(1,n_features) and print their scores\n",
    "* Ridge()\n",
    "* RidgeCV - get the best alpha\n",
    "* Demo: for alpha in list, plot coef chart\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "Our Scikit learn models expect all numbers, so we encode (low-cardinality) categorical strings\n",
    "\n",
    "Feature selection is a big deal\n",
    "\n",
    "There are n_features! combinations of features\n",
    "\n",
    "Univariate feature selection uses univariate statistics to pick features.\n",
    "* You can SelectKBest, SelectPercentile of best features, etc.\n",
    "* There are other types (like permutation importance). Details here https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "Regularization to add just enough bias that models don't go crazy.\n",
    "  * alphas range from =OLS line to =mean baseline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZnfcU7LRIWI"
   },
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzIVw96HRIS3"
   },
   "source": [
    "### Module 4: Logistic Regression \n",
    "\n",
    "Techniques used:\n",
    "* log_reg.coef_, .intercept_, .predict, .predict_proba\n",
    "* 2 ways to score:\n",
    "  * y_pred = model.predidct(X_val_scaled) --> accuracy_score(y_val, y_pred)\n",
    "  * model.score(X_val_scaled, y_val)\n",
    "\n",
    "\n",
    "Topics covered:\n",
    "* train/val/test split\n",
    "* majority class baseline\n",
    "* LogisticRegression() for baseline classification\n",
    "* Evaluation metric - accuracy_score\n",
    "* math breakdown - the sigmoid function\n",
    "* first good full pipeline -- encode, impute, scale, fit\n",
    "* LogisticRegressionCV()\n",
    "* plot coefficients\n",
    "* Kaggle submission code .to_csv()\n",
    "\n",
    "\n",
    "\n",
    "Notes:  \n",
    "There are 9 and 12 minute Aaron videos for the math of logistic regression\n",
    "\n",
    "Squishification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpYniNrLn_nk"
   },
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXh5Iq00oBe5"
   },
   "source": [
    "## Sprint 2 (Kaggle) -- Decision Trees, Random Forests, CrossVal, Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGdLZp6RoMyw"
   },
   "source": [
    "### Module 1: Decision Trees\n",
    "\n",
    "Techniques used:\n",
    "* pandas_profiling to find issues with the data\n",
    "* Plotly express - mapbox scatter\n",
    "* graphviz\n",
    "* itertools, cool function visualizing LogReg predictions vs DecisionTree ones\n",
    "\n",
    "\n",
    "Topics covered:\n",
    "* train_test_splitting train into train, val\n",
    "* Wrangle train-test-val the same way w a function\n",
    "  * Pass in X, make X.copy(), replace 0-nulls w NaNs, drop redundant columns, return X\n",
    "* various feature selections -- i.e. numericals + categoricals under 50\n",
    "* Pipeline to combine all the steps -- Encode, Impute, Scale, Fit, Predict\n",
    "* Get and plot coefficients -- harder w pipelines (no .coef_'s)\n",
    "  * access \"named_steps\"\n",
    "* DecisionTreeClassifier()\n",
    "* .feature_importances_\n",
    "* cool demos visualizing water pump and other predictions w different max depths\n",
    "\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "Reference resource: [The Quartz Guide to Bad Data](https://github.com/Quartz/bad-data-guide)\n",
    "\n",
    "Split train into train, val if test is already provided\n",
    "\n",
    "In a pipeline, you can grid search over all the parameters at once\n",
    "\n",
    "Pipelines let you write more concise code with fewer errors, and adds safety to the code because you have to go into the pipeline to mess anything up.\n",
    "\n",
    "Linear models have coefficients; trees have feature importances\n",
    "\n",
    "Assignment notebook has good \"real-worldish\" recommendations to explore data. It's feisty coding.\n",
    "  * also replace less common cardinalities with \"OTHER\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RHucqrEoR8c"
   },
   "source": [
    "### Module 2: Random Forests\n",
    "\n",
    "Techniques used:\n",
    "* asdf\n",
    "\n",
    "\n",
    "Topics covered:\n",
    "* split, wrangle as before\n",
    "* RandomForestClassifier() - pipeline w encode, impute\n",
    "* accessing things after pipeline\n",
    "  * encoder = named_steps['onehotencoder'] --> encoded = encoder.transform(X_train)\n",
    "* model.feature_importances_\n",
    "* ce.OrdinalEncoder()\n",
    "* extremely randomized trees - ExtraTreesClassifier, ExtraTreesRegressor\n",
    "* big demo - LogReg, DecTree, RandForest.\n",
    "  * interactive widget sliders and model comparisons\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "When you want to access something that's in a pipeline, you have to use pipeline.named_steps['nameinalllowercase']\n",
    "\n",
    "Random forest is bagging - \"bootstrap aggregating\" (which just means lots of small samples w replacement).  \n",
    "Gradient boost is boosting\n",
    "  * the former may be less sensitive to hyperparameters, the latter might score better once fit well\n",
    "\n",
    "ordinal encoding lets you use high-card cats\n",
    "\n",
    "With ordinal encoding, \"you can optionally pass in a mapping dict to give order to the classes rather than selecting integers at random -- this assumes the classes have inherent order\"\n",
    "\n",
    "selectKbest is for linear models\n",
    "\n",
    "permutation importances for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URqU2mpCoRhe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Unit_2_summary.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
